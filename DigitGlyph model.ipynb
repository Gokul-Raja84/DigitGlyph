{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIGIT GLYPH "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Hand Written Digit Recognition Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  DigitGlyph: Bridging Art and Technology for Handwritten Digit Recognition\n",
    "\n",
    "DigitGlyph is an innovative web application that brings together the world of artistic expression and cutting-edge technology. By leveraging the power of Convolutional Neural Networks (CNNs), DigitGlyph enables the recognition and classification of handwritten digits, creating a dynamic fusion between human creativity and machine intelligence.\n",
    "\n",
    "\n",
    "Access the Web App here : [DigitGlyph](https://gokul-raja84.github.io/DigitGlyph/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project Overview:\n",
    "\n",
    "At the core of DigitGlyph lies a 17-layer CNN model, meticulously designed for recognizing and classifying handwritten digits. Trained on the venerable MNIST dataset, consisting of 70,000 grayscale images, the model transforms pixelated strokes into meaningful digital interpretations. With a carefully crafted combination of Conv2D, MaxPooling2D, BatchNormalization, Dense, Flatten, and Dropout layers, the model achieves an impressive accuracy of 99.15% after 30 epochs of training. The input layer, housing 32 neurons, feeds into an output layer with 10 neurons, corresponding to the 10 distinct digit classes.\n",
    "\n",
    "- The model achieves an accuracy of 99.15%, showcasing its strong ability to accurately recognize and classify handwritten digits.It highlights the successful implementation of DigitGlyph, operating as a robust and potent tool for precise digit recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Handwritten Digits Classification using Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Conv2D,MaxPooling2D,Flatten,BatchNormalization\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "# This code implements a Convolutional Neural Network (CNN) to classify the MNIST dataset of handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Splitting the MNIST dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train,y_train),(x_test,y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring MNIST Dataset Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of x_train: (60000, 28, 28)\n",
      "Dimension of x_test: (10000, 28, 28)\n",
      "Dimension of y_test: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Printing the dimensions of the dataset arrays\n",
    "\n",
    "print(\"Dimension of x_train:\", x_train.shape)  # Displaying the shape of the training data\n",
    "print(\"Dimension of x_test:\", x_test.shape)    # Displaying the shape of the testing data\n",
    "print(\"Dimension of y_test:\", y_test.shape)    # Displaying the shape of the testing labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing a Single Image from the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "# This code displays a single image from the MNIST dataset along with its corresponding label.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Displaying a single image from the training dataset\n",
    "plt.imshow(x_train[4])  # Displaying the image using matplotlib\n",
    "plt.show()\n",
    "\n",
    "# Printing the corresponding label of the displayed image\n",
    "\n",
    "print(\"Label:\", y_train[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Parameters for the Neural Network Model\n",
    "\n",
    "This code sets up various parameters that will be used when constructing a neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting Up Parameters for MNIST Image Classification\n",
    "# This code defines essential parameters for an MNIST image classification model.\n",
    "\n",
    "# Defining the number of classes in the classification problem\n",
    "\n",
    "num_classes = 10  # MNIST dataset has 10 classes (digits 0 to 9)\n",
    "\n",
    "# Defining the number of training epochs\n",
    "\n",
    "epochs = 30\n",
    "\n",
    "# Defining image dimensions for preprocessing\n",
    "\n",
    "img_rows = 28  # Height of each image in pixels\n",
    "img_cols = 28  # Width of each image in pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization and Label Encoding for MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code performs data normalization and label encoding for the MNIST dataset.\n",
    "\n",
    "# Normalizing image data to a range of [0, 1]\n",
    "\n",
    "x_train = x_train.astype(float)\n",
    "x_train /= 255.0  # Scaling pixel values to be between 0 and 1\n",
    "x_test = x_test.astype(float)\n",
    "x_test /= 255.0   # Scaling pixel values to be between 0 and 1\n",
    "\n",
    "# Importing utility for one-hot encoding\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Performing one-hot encoding for label data\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype='float32')  # Converting labels to one-hot encoded vectors\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype='float32')    # Converting labels to one-hot encoded vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting Image Data for Neural Network Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code adjusts the format of image data based on the preferred channel order for neural network input.\n",
    "\n",
    "# Checking the image data format preference\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    \n",
    "    # Reshaping image data for 'channels_first' format (channel first, then image dimensions)\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)  # Input shape for the neural network\n",
    "    \n",
    "else:\n",
    "    # Reshaping image data for 'channels_last' format (image dimensions first, then channel)\n",
    "    \n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)  # Input shape for the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Convolutional Neural Network (CNN) Model for MNIST Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code constructs a Convolutional Neural Network (CNN) model for classifying the MNIST dataset.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dropout, Flatten, Dense\n",
    "\n",
    "# Creating a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Adding Convolutional and Pooling layers\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, kernel_size=5, strides=2, padding='same', activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "# Flattening and adding Dense layers\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the Convolutional Neural Network (CNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# This code compiles the constructed CNN model for the MNIST classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 11, 11, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 11, 11, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 32)          25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 6, 6, 32)          128       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 4, 4, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 2, 2, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 2, 2, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 1, 1, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 1, 1, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 1, 1, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 194,762\n",
      "Trainable params: 194,250\n",
      "Non-trainable params: 512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Convolutional Neural Network (CNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 38s 638us/sample - loss: 0.4194 - accuracy: 0.8753 - val_loss: 0.7520 - val_accuracy: 0.7056\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 42s 695us/sample - loss: 0.1178 - accuracy: 0.9689 - val_loss: 0.0510 - val_accuracy: 0.9827\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 35s 579us/sample - loss: 0.0803 - accuracy: 0.9780 - val_loss: 0.0377 - val_accuracy: 0.9872\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 39s 642us/sample - loss: 0.0612 - accuracy: 0.9832 - val_loss: 0.0392 - val_accuracy: 0.9889\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 44s 728us/sample - loss: 0.0526 - accuracy: 0.9855 - val_loss: 0.0264 - val_accuracy: 0.9912\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 43s 709us/sample - loss: 0.0457 - accuracy: 0.9875 - val_loss: 0.0266 - val_accuracy: 0.9911\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 41s 680us/sample - loss: 0.0395 - accuracy: 0.9894 - val_loss: 0.0303 - val_accuracy: 0.9910\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 36s 599us/sample - loss: 0.0361 - accuracy: 0.9902 - val_loss: 0.0411 - val_accuracy: 0.9889\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 35s 592us/sample - loss: 0.0329 - accuracy: 0.9908 - val_loss: 0.0306 - val_accuracy: 0.9905\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 40s 668us/sample - loss: 0.0290 - accuracy: 0.9916 - val_loss: 0.0260 - val_accuracy: 0.9921\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 51s 851us/sample - loss: 0.0306 - accuracy: 0.9911 - val_loss: 0.0310 - val_accuracy: 0.9916\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 58s 965us/sample - loss: 0.0268 - accuracy: 0.9926 - val_loss: 0.0265 - val_accuracy: 0.9935\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 41s 683us/sample - loss: 0.0243 - accuracy: 0.9933 - val_loss: 0.0232 - val_accuracy: 0.9935\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 40s 663us/sample - loss: 0.0247 - accuracy: 0.9930 - val_loss: 0.0248 - val_accuracy: 0.9931\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 38s 641us/sample - loss: 0.0225 - accuracy: 0.9937 - val_loss: 0.0282 - val_accuracy: 0.9922\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 40s 674us/sample - loss: 0.0212 - accuracy: 0.9942 - val_loss: 0.0303 - val_accuracy: 0.9922\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 41s 682us/sample - loss: 0.0204 - accuracy: 0.9943 - val_loss: 0.0346 - val_accuracy: 0.9918\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 41s 676us/sample - loss: 0.0179 - accuracy: 0.9949 - val_loss: 0.0223 - val_accuracy: 0.9939\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 38s 641us/sample - loss: 0.0174 - accuracy: 0.9951 - val_loss: 0.0246 - val_accuracy: 0.9928\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 37s 614us/sample - loss: 0.0189 - accuracy: 0.9946 - val_loss: 0.0272 - val_accuracy: 0.9934\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 37s 613us/sample - loss: 0.0173 - accuracy: 0.9952 - val_loss: 0.0236 - val_accuracy: 0.9930\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 37s 619us/sample - loss: 0.0157 - accuracy: 0.9957 - val_loss: 0.0231 - val_accuracy: 0.9945\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 37s 617us/sample - loss: 0.0168 - accuracy: 0.9952 - val_loss: 0.0231 - val_accuracy: 0.9935\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 37s 617us/sample - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.0270 - val_accuracy: 0.9931\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 38s 628us/sample - loss: 0.0134 - accuracy: 0.9961 - val_loss: 0.0275 - val_accuracy: 0.9942\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 37s 621us/sample - loss: 0.0124 - accuracy: 0.9965 - val_loss: 0.0290 - val_accuracy: 0.9930\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 38s 632us/sample - loss: 0.0128 - accuracy: 0.9963 - val_loss: 0.0361 - val_accuracy: 0.9907\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 35s 590us/sample - loss: 0.0127 - accuracy: 0.9962 - val_loss: 0.0242 - val_accuracy: 0.9946\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 35s 591us/sample - loss: 0.0126 - accuracy: 0.9965 - val_loss: 0.0304 - val_accuracy: 0.9927\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 35s 590us/sample - loss: 0.0109 - accuracy: 0.9968 - val_loss: 0.0360 - val_accuracy: 0.9915\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8ed0c351d0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code trains the compiled CNN model using the training dataset and validates it on the test dataset.\n",
    "\n",
    "# Training the CNN model\n",
    "\n",
    "model.fit(x_train, y_train,             # Training data and labels\n",
    "          batch_size=batch_size,       # Number of samples per gradient update\n",
    "          epochs=epochs,               # Number of times to iterate over the training dataset\n",
    "          validation_data=(x_test, y_test))  # Validation data for evaluating model performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Trained Convolutional Neural Network (CNN) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 2s 224us/sample - loss: 0.0360 - accuracy: 0.9915\n",
      "Score is : 0.03601376320495292\n",
      "Accuracy : 0.9915\n"
     ]
    }
   ],
   "source": [
    "# This code evaluates the trained CNN model on the test dataset and prints the score and accuracy.\n",
    "\n",
    "# Evaluating the trained CNN model on the test dataset\n",
    "score, accuracy = model.evaluate(x_test, y_test)  # Evaluating the model's performance\n",
    "\n",
    "# Printing the evaluation results\n",
    "print(\"Score:\", score)         # Displaying the model's score\n",
    "print(\"Accuracy:\", accuracy)   # Displaying the model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code saves the trained CNN model's architecture and weights to files.\n",
    "\n",
    "# Converting the trained CNN model's architecture to JSON format\n",
    "\n",
    "model_json = model.to_json()\n",
    "\n",
    "# Saving the model's architecture to a JSON file\n",
    "\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "\n",
    "# Saving the trained model's weights to a HDF5 file\n",
    "\n",
    "model.save_weights(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting and Saving Keras Model to TensorFlow.js Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflowjs as tfjs\n",
    "\n",
    "# Convert and save the Keras model to TensorFlow.js format\n",
    "\n",
    "tfjs.converters.save_keras_model(model,'C:/Users/GOKUL RAJA/PycharmProjects/pythonProject/model_tfjs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "DigitGlyph isn't just a tool for recognizing digits; it's fusion between creative expression and technological innovation. By transforming handwritten strokes into meaningful classifications, DigitGlyph transcends traditional boundaries and showcases the potential of harmonizing art and technology. Join us on this captivating journey at [DigitGlyph Web App](https://gokul-raja84.github.io/DigitGlyph/) and experience the magic firsthand.\n",
    "\n",
    "\n",
    "### Connect with Us\n",
    "\n",
    "For feedback, suggestions, or collaborations, feel free to connect with us on [GitHub](https://github.com/Gokul-Raja84) or [Contact Me](mailto:gokulraja840@gmail.com). Let's explore the realms of art and technology together!\n",
    "\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "Feel Free to Explore and Experiment: I encourage you to delve into the codebase, tinker with various architectures and hyperparameters, and contribute to enhancing the model's accuracy and performance. Your insights and innovations can play a crucial role in advancing this project !\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
